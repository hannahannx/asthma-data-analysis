{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Preparation - accessing quality and cleaning data",
   "id": "4061525fe7df94bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load and and Present Data ",
   "id": "c14370a1cef710bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T06:59:15.862453Z",
     "start_time": "2025-01-06T06:59:15.266962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "import plotly.express as px\n",
    "from scipy import stats\n",
    "\n",
    "# Load the data set \n",
    "df = pd.read_csv('../data/raw_data/Children-and-young-ppl-asthma-organisational-audit-2019-20-Data.csv')\n",
    "#Pivot Table to describe the dataset\n",
    "print(\"Original Data\")\n",
    "df.info()\n",
    "#Returning how many rows in the data frame \n",
    "print(\"\\n The amount of organisations in the raw data set is\",df.count().values[0],\".\") "
   ],
   "id": "a430156b7f6ef0f0",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '__version__' from 'sklearn' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[71], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimpute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SimpleImputer\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mplotly\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexpress\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpx\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m stats\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/impute/__init__.py:8\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Authors: The scikit-learn developers\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# SPDX-License-Identifier: BSD-3-Clause\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_base\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MissingIndicator, SimpleImputer\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_knn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KNNImputer\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m typing\u001B[38;5;241m.\u001B[39mTYPE_CHECKING:\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;66;03m# Avoid errors in type checkers (e.g. mypy) for experimental estimators.\u001B[39;00m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;66;03m# TODO: remove this check once the estimator is no longer experimental.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/impute/_base.py:14\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mma\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mma\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sparse \u001B[38;5;28;01mas\u001B[39;00m sp\n\u001B[0;32m---> 14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseEstimator, TransformerMixin, _fit_context\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_mask\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _get_mask\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_missing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_pandas_na, is_scalar_nan\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/base.py:16\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m defaultdict\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_config\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config_context, get_config\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexceptions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InconsistentVersionWarning\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name '__version__' from 'sklearn' (unknown location)"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "607d1c26ac53998c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Quality Check and Cleaning - Irrelevant Columns",
   "id": "c0e839c1fe47145b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Dropping irrelevant columns \n",
    "#Dropping columns with contain the following keywords which are irrelevant for this data analysis\n",
    "previous_col_size= len(df.columns)\n",
    "valsToRemove = [\"WTE\",\"q8\",\"q7\",\"q6\",\"q5\",\"q4\",\"q3\",\"othernotlisted\",\"cat\"]\n",
    "listOfColumns = []\n",
    "\n",
    "for column in df.columns:\n",
    "    # converting data type for integer and float to numeric values \n",
    "    try:\n",
    "        #+5 IS BECAUSE THE FIRST 5 COLUMNS ARE KNOWN TO BE NON-NUMERICAL\n",
    "        df[column + 5] = pd.to_numeric(df[column], errors='coerce')  # Convert to numeric if possible\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if any(sub in column for sub in valsToRemove):\n",
    "        listOfColumns.append(column)\n",
    "    column_name = df[column].astype(str)\n",
    "    wteString = column_name.str.contains(\"|\".join(valsToRemove), case=False, na=False)\n",
    "    if wteString.any():\n",
    "        listOfColumns.append(column) \n",
    "df = df.drop(columns = listOfColumns)\n",
    "succeeding_col_size = len(df.columns)\n",
    "end_col_size = previous_col_size - succeeding_col_size\n",
    "print(df)\n",
    "print(\"There were\", end_col_size,\"irrelevant columns in the raw data set\")"
   ],
   "id": "10b38c405a589fe3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Cleaning - Missing Values ",
   "id": "406e02a0cab32e92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Handling the different types of na values \"-, n/a , null\"\n",
    "df = df.replace(['-','1-7'],np.nan)\n",
    "\n",
    "#Number of missing values in whole dataset\n",
    "total_missing_val= df.isna().sum().sum()\n",
    "\n",
    "# How many NA values are in a row\n",
    "print(\"EMPTY ROWS VALUES\")\n",
    "print(df.isna().sum(axis=1))\n",
    "\n",
    "# How many NA values are in a column\n",
    "print(\"\\nEMPTY COLUMNS VALUES\")\n",
    "print(df.isna().sum(axis=0))\n",
    "\n",
    "# Separate numeric and string columns\n",
    "numeric_col = [col for col in df.columns if df[col].dtypes in ['int64', 'float64']]\n",
    "string_columns = [col for col in df.columns if df[col].dtypes == 'object']\n",
    "\n",
    "# Missing value analysis - EDA \n",
    "figure = px.imshow(df.isna(), color_continuous_scale=\"pinkyl\",width=1000,height=600)\n",
    "figure.update_layout(title=\"Missing Value Analysis on Dataset\",margin=dict(t=50, b=50, l=50, r=50),)\n",
    "figure.show()\n",
    "\n",
    "# Filling NA values using simple impute and mode(most frequent)\n",
    "\n",
    "# Impute numeric columns with mean\n",
    "if numeric_col:\n",
    "    numeric_imputer = SimpleImputer(strategy='mean')\n",
    "    df[numeric_col] = numeric_imputer.fit_transform(df[numeric_col])\n",
    "\n",
    "# Impute string columns with the most frequent value\n",
    "if string_columns:\n",
    "    string_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    df[string_columns] = string_imputer.fit_transform(df[string_columns])\n",
    "\n",
    "print(\"\\n \\n\")    \n",
    "print(df)\n",
    "print(\"Total missing values that were replaced by SimpleImputer\", total_missing_val )\n",
    "\n",
    "# After\n",
    "# How many NA values are in a row\n",
    "print(\"EMPTY ROWS VALUES\")\n",
    "print(df.isna().sum(axis=1))\n",
    "\n",
    "# How many NA values are in a column\n",
    "print(\"\\nEMPTY COLUMNS VALUES\")\n",
    "print(df.isna().sum(axis=0))\n"
   ],
   "id": "c8f81506272c5059"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Quality Check and Cleaning - Duplicate Values",
   "id": "d0bb49e8e4cea522"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Duplicate values\n",
    "has_duplicates = df.duplicated().sum()\n",
    "print(\"The number of duplicate organisations are \",has_duplicates) "
   ],
   "id": "57d131cf1c7f4186"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Quality Check - Data Consistency",
   "id": "381c09e92de516"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Returns data type for the df       \n",
    "print(df.dtypes)"
   ],
   "id": "532002f390b3750e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Cleaning - Data Usability",
   "id": "d362a4fefef461d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Changing data frame column names\n",
    "col_names = [\"Hospital Code\",\"Hospital Name\",\"Trust Name\",\"Country\",\"Emergency Medical Admissions\",\"Emergency Repository Admissions\",\"Emergency Asthma Admissions\", \"Paediatric Admissions\", \"Repository Admissions\", \"Paediatric Asthma Admissions Per 1000 Admissions\",\"Total Paediatric Beds For Asthma Patients\",\"Paediatric HDU Present?\", \"Total Paediatric HDU Beds\",\"Paediatric ICU Present?\",\"Total Paediatric ICU Beds\", \"FY1/2 - Total Repository Staff Posts\",\"FY1/2 - Staff Posts Per 100 Repository Paediatric Admissions\", \"FY1/2 - Staff Posts Per 100 Asthma Paediatric Admissions\", \"FY1/2 -Filled Respiratory Staff Posts\",\"ST1/2 - Total Repository Staff Posts\",\"ST1/2 - Staff Posts Per 100 Repository Paediatric Admissions\",\"ST1/2 - Staff Posts Per 100 Asthma Paediatric Admissions\", \"ST1/2 -Filled Respiratory Staff Posts\",\"ST3+ - Total Repository Staff Posts\",\"ST3+ - Staff Posts Per 100 Repository Paediatric Admissions\",\"ST3+ - Staff Posts Per 100 Asthma Paediatric Admissions\", \"ST3+ -Filled Respiratory Staff Posts\",\"Repository Team Paediatric Consultant Filled Staff Post\",\" Paediatric Consultant Repository Admissions per 100 Admissions\",\"Paediatric Consultant Asthma Admissions per 100 Admissions\",\"Repository Team Paediatric Repository Consultant Filled Staff Post\",\" Paediatric Repository Consultant Repository Admissions per 100 Admissions\",\"Paediatric Repository Consultant Asthma Admissions per 100 Admissions\",\"Repository Team Associate Specialist Filled Staff Post\",\" Associate Specialist Repository Admissions per 100 Admissions\",\"Associate Specialist Asthma Admissions per 100 Admissions\",\"Repository Team Staff Grade Filled Staff Post\",\" Staff Grade Repository Admissions per 100 Admissions\",\"Staff Grade Asthma Admissions per 100 Admissions\",\"Repository Team Nurse Consultant Filled Staff Post\",\" Nurse Consultant Repository Admissions per 100 Admissions\",\"Nurse Consultant Asthma Admissions per 100 Admissions\", \"Repository Team Specialist Repository Filled Staff Post\",\" Specialist Repository Repository Admissions per 100 Admissions\",\" Specialist Repository Asthma Admissions per 100 Admissions\",\"Repository Team Paediatric Psychologist Filled Staff Post\",\" Paediatric Psychologist Repository Admissions per 100 Admissions\",\"Paediatric Psychologist Asthma Admissions per 100 Admissions\",\"Repository Team Paediatric Pharmacist Filled Staff Post\",\" Paediatric Pharmacist Repository Admissions per 100 Admissions\",\"Paediatric Pharmacist Asthma Admissions per 100 Admissions\", \"FY1/2 - Unfilled Staff Posts in Repository Team\",\"ST1/2 - Unfilled Staff Posts in Repository Team\",\"ST3+ - Unfilled Staff Posts in Repository Team\", \"Unfilled Paediatric Consultant Staff Posts\", \"Unfilled Paediatric Repository Consultant Staff Posts\",\"Unfilled Associate Specialist Staff Posts\",\"Unfilled Staff Grade Staff Posts\", \"Unfilled Asthma Nurse Consultant Staff Posts\",\"Unfilled  Nurse Consultant Staff Posts\", \"Unfilled Nurse Specialist Repository Staff Posts\",\"Unfilled Paediatric Psychologist Staff Posts\",\"Unfilled Paediatric Pharmacist Staff Posts\"]\n",
    "update_col_names = [f\"{name}\" for name in col_names]\n",
    "df.columns = update_col_names\n",
    "list_of_asthma = []\n",
    "\n",
    "for name in col_names:\n",
    "    #if the value contains the str \"paediatric asthma\" then add to list\n",
    "    if \"Asthma Admission\" in name:\n",
    "        list_of_asthma.append(name)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "        \n",
    "print(df)\n",
    "print(list_of_asthma)"
   ],
   "id": "e724a05fc596b8c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Cleaning - Removing Outliers ",
   "id": "ddc93be3fad2e29f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['number'])\n",
    "\n",
    "# Filter out columns with all NaN or constant values\n",
    "valid_cols = numeric_cols.loc[:, numeric_cols.nunique(dropna=True) > 1]\n",
    "\n",
    "#Diplaying outliers for each colum in the valid columns \n",
    "for col in valid_cols:\n",
    "    z = np.abs(stats.zscore(df[col]))\n",
    "    print(z)\n",
    "\n",
    "threshold_z = 3\n",
    "\n",
    "outlier_indices = np.where(z > threshold_z)[0]\n",
    "no_outliers = df.drop(outlier_indices)\n",
    "print(\"Original DataFrame Shape:\", df.shape)\n",
    "print(\"DataFrame Shape after Removing Outliers:\", no_outliers.shape)\n",
    "df"
   ],
   "id": "3910d3eb74124f8f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "65b55da7167c07c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Cleaned Data Frame",
   "id": "ca1d60aa687cbe85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Saving the cleaned data set into a new file \n",
    "df.to_csv(\"../data/cleaned_data/cleaned_dataset.csv\", index=False)"
   ],
   "id": "271a801d76e773ed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
